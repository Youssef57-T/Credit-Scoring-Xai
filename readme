
## Project Overview

The project is credit risk analysis with use of several machine learning models and eXplainable AI (XAI) techniques. Several classifiers and explanation tools were experimented with by the team to gauge both model performance and explainability. Various models and explanations were provided by every member, and comparisons were done using several different evaluation metrics.

## Team Contributions

### Youssef

*Logistic Regression

* Accuracy: 0.5371
  * Techniques: Coefficient Importance Plot, Odds Ratios, Confusion Matrix
* Naive Bayes

  * Accuracy: 0.5425
* SVM

  * Techniques: LIME, SHAP, Odds Ratio, Confusion Matrix

### Yasmin

* Decision Tree

  * Accuracy: 0.6534
  * Precision: 0.6487
  * Recall: 0.6534
  * F1 Score: 0.6503
  * Techniques: Decision Path, Feature Importance, SHAP, LIME, Global Surrogate Model
* AdaBoost

  * Validation Accuracy: 0.6209
* Test Accuracy: 0.6258
  * Techniques: Feature Importances, Permutation Importance, LIME Explainer, Surrogate Model, LOFO Importance
* Random Forest

  * Validation Accuracy: 0.7246
  * Techniques: Feature Importance (Gini-based), Permutation Importance, PDP, ICE, Decision Paths, SHAP
* Multi-Class Logistic Regression

  * Validation Accuracy: 0.5246
  * Techniques: PDP, GLM Model, SHAP

### Mommen

* Random Forest

  * Accuracy: 0.80
* Techniques: Feature Importance, PDP, Permutation Importance, LIME
* **XGBoost**

  * Accuracy: 0.76
  * Techniques: Feature Importance, PDP, Permutation Importance, LIME, SHAP
* Bagging Classifier (using DT)

  * Accuracy: 0.80
  * Techniques: Feature Importance, PDP, Permutation Importance, LIME, SHAP

### Raneem

* SVM, Neural Network, Random Forest, XGBoost

  * Techniques: SHAP, LIME, PDP, Permutation, ROC Curve, ALE (for SVM and NN)

* Neural Network Results:

* Confusion Matrix:

    [[ 61  10  58]
     [  7 143  89]
     [ 31  89 220]]
  * Accuracy: 0.60

* Random Forest Results:

  * Confusion Matrix:


    [[39  0  23]
[ 0  83  21]
    [ 9  15 163]]

  * Accuracy: 0.81

* XGBoost Results:

  * Confusion Matrix:
    [[1188   26  522]
     [  72 2432  847]
     [ 411  691 4421]]

* Accuracy: 0.76


## How to Run the Code

### Prerequisites

* Python 3.8 or later
* Install dependencies with:


  pip install -r requirements.txt

### Installation

1. Clone the repository:

   git clone <repository_url>
   cd <repository_directory>
   
2. Install the dependencies:
   pip install -r requirements.txt


### Running the Models

Go to the relevant script and run:
python logistic_regression.py
python naive_bayes.py
python decision_tree.py
python random_forest.py
python xgboost.py
python svm.py
python neural_network.py

## XAI Techniques Used

* SHAP (SHapley Additive exPlanations)
* LIME(Local Interpretable Model-Agnostic Explanations)
* PDP (Partial Dependence Plots)
* ICE(Individual Conditional Expectation)
* Permutation Importance
* Feature Importance (Gini, Coefficients, etc.)
* Odds Ratios(Logistic Regression)
* Confusion Matrix
* ROC Curve
* ALE (Accumulated Local Effects)(ann,svm raneem)
* LOFO Importance
* Decision Paths
* Global Surrogate Models


## Required Imports
python
# SHAP
import shap
from shap import TreeExplainer, KernelExplainer, summary_plot, force_plot, dependence_plot

# LIME
import lime
from lime import lime_tabular
from lime.lime_tabular import LimeTabularExplainer

# PDP and ICE
from sklearn.inspection import partial_dependence, PartialDependenceDisplay

# Permutation Importance
from sklearn.inspection import permutation_importance

# Feature Importance
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
plot_importance = lambda: plot_importance  

# Logistic Regression: Odds Ratios
import numpy as np
import statsmodels.api as sm

# Confusion Matrix and ROC Curve
from sklearn.metrics import confusion_matrix, roc_curve, auc, RocCurveDisplay

# ALE
from alibi.explainers import ALE, plot_ale


