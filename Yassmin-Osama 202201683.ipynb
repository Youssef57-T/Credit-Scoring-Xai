{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import (roc_auc_score, confusion_matrix, roc_curve,\n",
        "                            precision_recall_curve, average_precision_score)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import shap\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    plt.style.use('seaborn')\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
        "\n",
        "data = pd.read_csv('/content/train.csv')\n",
        "\n",
        "def clean_data(df):\n",
        "    missing_values = ['NA', 'N/A', 'NaN', np.nan, '', ' ', '_', '-', 'unknown', 'Unknown']\n",
        "    df = df.replace(missing_values, np.nan)\n",
        "\n",
        "    numeric_cols = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts',\n",
        "                   'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',\n",
        "                   'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries',\n",
        "                   'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Total_EMI_per_month',\n",
        "                   'Amount_invested_monthly', 'Monthly_Balance']\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.replace(r'[^0-9.-]', '', regex=True)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    def convert_history(history):\n",
        "        if pd.isna(history):\n",
        "            return np.nan\n",
        "        try:\n",
        "            if isinstance(history, (int, float)):\n",
        "                return history\n",
        "            history = str(history).lower()\n",
        "            if 'years' in history and 'months' in history:\n",
        "                years = int(history.split('years')[0].strip())\n",
        "                months = int(history.split('and')[1].split('months')[0].strip())\n",
        "                return years * 12 + months\n",
        "            elif 'years' in history:\n",
        "                return int(history.split('years')[0].strip()) * 12\n",
        "            else:\n",
        "                return np.nan\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    if 'Credit_History_Age' in df.columns:\n",
        "        df['Credit_History_Age_Months'] = df['Credit_History_Age'].apply(convert_history)\n",
        "        df.drop('Credit_History_Age', axis=1, inplace=True)\n",
        "\n",
        "    if 'Payment_Behaviour' in df.columns:\n",
        "        df['Payment_Behaviour'] = df['Payment_Behaviour'].str.replace('!@9#%8', 'Unknown')\n",
        "        df['Payment_Behaviour'] = df['Payment_Behaviour'].str.replace('__', '_')\n",
        "        df['Payment_Behaviour'] = df['Payment_Behaviour'].str.replace(' ', '_')\n",
        "        df['Payment_Behaviour'] = df['Payment_Behaviour'].str.lower()\n",
        "\n",
        "    if 'Credit_Mix' in df.columns:\n",
        "        df['Credit_Mix'] = df['Credit_Mix'].replace(['_', 'Standard', 'standard'], 'Good')\n",
        "        df['Credit_Mix'] = df['Credit_Mix'].str.lower()\n",
        "\n",
        "    threshold = len(df) * 0.7\n",
        "    df = df.dropna(thresh=threshold, axis=1)\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            if df[col].isna().sum() > 0:\n",
        "                df[col] = df[col].fillna('missing')\n",
        "        else:\n",
        "            if df[col].isna().sum() > 0:\n",
        "                df[f'{col}_missing'] = df[col].isna().astype(int)\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    return df\n",
        "\n",
        "data = clean_data(data)\n",
        "\n",
        "def create_target(df):\n",
        "    df['Payment_Score'] = 0\n",
        "\n",
        "    payment_map = {\n",
        "        'low_spent_small_value_payments': 2,\n",
        "        'low_spent_medium_value_payments': 1,\n",
        "        'low_spent_large_value_payments': 0,\n",
        "        'high_spent_small_value_payments': 0,\n",
        "        'high_spent_medium_value_payments': -1,\n",
        "        'high_spent_large_value_payments': -2,\n",
        "        'missing': -1,\n",
        "        'unknown': -1\n",
        "    }\n",
        "\n",
        "    if 'Payment_Behaviour' in df.columns:\n",
        "        df['Payment_Score'] += df['Payment_Behaviour'].map(payment_map).fillna(-1)\n",
        "\n",
        "    credit_mix_map = {\n",
        "        'good': 2,\n",
        "        'standard': 1,\n",
        "        'bad': -2,\n",
        "        'missing': -1\n",
        "    }\n",
        "\n",
        "    if 'Credit_Mix' in df.columns:\n",
        "        df['Payment_Score'] += df['Credit_Mix'].map(credit_mix_map).fillna(-1)\n",
        "\n",
        "    if 'Num_of_Delayed_Payment' in df.columns:\n",
        "        df['Payment_Score'] += np.where(df['Num_of_Delayed_Payment'] == 0, 2,\n",
        "                                      np.where(df['Num_of_Delayed_Payment'] <= 3, 1,\n",
        "                                              np.where(df['Num_of_Delayed_Payment'] <= 7, -1, -2)))\n",
        "\n",
        "    df['target'] = (df['Payment_Score'] > df['Payment_Score'].quantile(0.4)).astype(int)\n",
        "\n",
        "    df['target_explanation'] = np.where(df['target'] == 1,\n",
        "                                       \"Good credit risk based on payment behavior and credit mix\",\n",
        "                                       \"Poor credit risk based on payment behavior and credit mix\")\n",
        "\n",
        "    return df\n",
        "\n",
        "data = create_target(data)\n",
        "\n",
        "def create_features(df):\n",
        "    if 'Annual_Income' in df.columns and 'Outstanding_Debt' in df.columns:\n",
        "        df['Debt_to_Income'] = np.log1p(df['Outstanding_Debt']) / (np.log1p(df['Annual_Income']) + 1e-6)\n",
        "\n",
        "    if 'Credit_Utilization_Ratio' in df.columns:\n",
        "        df['Utilization_Ratio'] = np.clip(df['Credit_Utilization_Ratio'] / 100, 0, 1)\n",
        "\n",
        "    if 'Num_of_Delayed_Payment' in df.columns and 'Num_of_Loan' in df.columns:\n",
        "        df['Delay_Ratio'] = (df['Num_of_Delayed_Payment'] + 1) / (df['Num_of_Loan'] + 3)\n",
        "\n",
        "    if 'Total_EMI_per_month' in df.columns and 'Monthly_Inhand_Salary' in df.columns:\n",
        "        df['EMI_to_Income'] = np.log1p(df['Total_EMI_per_month']) / (np.log1p(df['Monthly_Inhand_Salary']) + 1e-6)\n",
        "\n",
        "    if 'Age' in df.columns:\n",
        "        df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 35, 45, 55, 65, 100],\n",
        "                                labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
        "\n",
        "    if 'Credit_History_Age_Months' in df.columns:\n",
        "        df['Credit_History_Length'] = pd.cut(df['Credit_History_Age_Months'],\n",
        "                                           bins=[0, 12, 36, 60, 120, 240, 600],\n",
        "                                           labels=['<1yr', '1-3yrs', '3-5yrs',\n",
        "                                                  '5-10yrs', '10-20yrs', '20+yrs'])\n",
        "\n",
        "    if 'Num_Credit_Card' in df.columns and 'Num_of_Loan' in df.columns:\n",
        "        df['Credit_Products_Count'] = df['Num_Credit_Card'] + df['Num_of_Loan']\n",
        "\n",
        "    return df\n",
        "\n",
        "data = create_features(data)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "numeric_features = ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts',\n",
        "                   'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date',\n",
        "                   'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Outstanding_Debt',\n",
        "                   'Credit_Utilization_Ratio', 'Credit_History_Age_Months', 'Total_EMI_per_month',\n",
        "                   'Amount_invested_monthly', 'Monthly_Balance', 'Debt_to_Income',\n",
        "                   'Utilization_Ratio', 'Delay_Ratio', 'EMI_to_Income']\n",
        "\n",
        "categorical_features = ['Age_Group', 'Credit_History_Length', 'Payment_Behaviour', 'Credit_Mix']\n",
        "\n",
        "features = numeric_features + [f for f in categorical_features if f in data.columns]\n",
        "target = 'target'\n",
        "\n",
        "data = data.dropna(subset=features + [target])\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_features:\n",
        "    if col in data.columns:\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train[numeric_features])\n",
        "X_test_scaled = scaler.transform(X_test[numeric_features])\n",
        "\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "with open('label_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoders, f)\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
        "    'SVM': CalibratedClassifierCV(SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42))\n",
        "}\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[(name, model) for name, model in models.items()],\n",
        "    voting='soft')\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    print(f\"{name} trained\")\n",
        "\n",
        "voting_clf.fit(X_train_scaled, y_train)\n",
        "print(\"Voting classifier trained\")\n",
        "\n",
        "with open('credit_score_models.pkl', 'wb') as f:\n",
        "    pickle.dump({'models': models, 'voting': voting_clf}, f)\n",
        "\n",
        "def evaluate_model(model, X, y, model_name=\"Model\"):\n",
        "    y_pred = model.predict(X)\n",
        "    y_prob = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Bad', 'Good'],\n",
        "                yticklabels=['Bad', 'Good'])\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    auc = roc_auc_score(y, y_prob)\n",
        "    print(f\"{model_name} AUC: {auc:.3f}\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y, y_prob)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{model_name} ROC Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y, y_prob)\n",
        "    ap = average_precision_score(y, y_prob)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, label=f'{model_name} (AP = {ap:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'{model_name} Precision-Recall Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return auc\n",
        "\n",
        "print(\"Individual Model Performance:\")\n",
        "model_performance = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} Evaluation:\")\n",
        "    auc = evaluate_model(model, X_test_scaled, y_test, name)\n",
        "    model_performance[name] = auc\n",
        "\n",
        "print(\"\\nVoting Classifier Evaluation:\")\n",
        "voting_auc = evaluate_model(voting_clf, X_test_scaled, y_test, \"Voting Classifier\")\n",
        "model_performance['Voting'] = voting_auc\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "lr_coef = models['Logistic Regression'].coef_[0]\n",
        "sorted_idx = np.argsort(np.abs(lr_coef))[::-1]\n",
        "plt.barh(np.array(features)[sorted_idx][:15], lr_coef[sorted_idx][:15])\n",
        "plt.title('Top 15 Logistic Regression Feature Importance (Absolute Coefficients)')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "rf_importance = models['Random Forest'].feature_importances_\n",
        "sorted_idx = np.argsort(rf_importance)[::-1]\n",
        "plt.barh(np.array(features)[sorted_idx][:15], rf_importance[sorted_idx][:15])\n",
        "plt.title('Top 15 Random Forest Feature Importance')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "explainer = shap.TreeExplainer(models['Random Forest'])\n",
        "shap_values = explainer.shap_values(X_test[features])\n",
        "shap.summary_plot(shap_values[1], X_test[features], feature_names=features, plot_type=\"bar\")\n",
        "\n",
        "def create_scorecard(model, features, pdo=50, score=600, odds=20):\n",
        "    if not hasattr(model, 'coef_'):\n",
        "        print(\"Warning: Model doesn't have coefficients. Using Random Forest feature importances instead.\")\n",
        "        coef = model.feature_importances_\n",
        "        intercept = 0\n",
        "    else:\n",
        "        coef = model.coef_[0]\n",
        "        intercept = model.intercept_[0]\n",
        "\n",
        "    factor = pdo / np.log(2)\n",
        "    offset = score - factor * np.log(odds)\n",
        "\n",
        "    scorecard = pd.DataFrame({'Feature': features, 'Coefficient': coef})\n",
        "\n",
        "    total_effect = intercept + sum(coef)\n",
        "    if total_effect == 0:\n",
        "        total_effect = 1\n",
        "\n",
        "    scorecard['Points'] = scorecard['Coefficient'].apply(\n",
        "        lambda x: -x * (factor / (np.log(2) * total_effect)))\n",
        "\n",
        "    scorecard['Rank'] = scorecard['Coefficient'].abs().rank(ascending=False)\n",
        "\n",
        "    return scorecard, offset\n",
        "\n",
        "scorecard, base_score = create_scorecard(models['Logistic Regression'], features)\n",
        "print(\"\\nScorecard:\")\n",
        "print(scorecard.sort_values('Points', ascending=False).head(10))\n",
        "print(f\"Base Score: {base_score:.1f}\")\n",
        "\n",
        "scorecard.to_csv('enhanced_credit_scorecard.csv', index=False)\n",
        "\n",
        "def calculate_score(row, scorecard, base_score, min_score=300, max_score=850):\n",
        "    score = base_score\n",
        "    for _, sc_row in scorecard.iterrows():\n",
        "        feature = sc_row['Feature']\n",
        "        if feature in row:\n",
        "            score += sc_row['Points'] * row[feature]\n",
        "\n",
        "    score = max(min_score, min(max_score, score))\n",
        "    return score\n",
        "\n",
        "data['Score'] = data.apply(lambda row: calculate_score(row, scorecard, base_score), axis=1)\n",
        "\n",
        "def get_rating(score):\n",
        "    if score >= 800:\n",
        "        return 'Excellent (800+)'\n",
        "    elif score >= 740:\n",
        "        return 'Very Good (740-799)'\n",
        "    elif score >= 670:\n",
        "        return 'Good (670-739)'\n",
        "    elif score >= 580:\n",
        "        return 'Fair (580-669)'\n",
        "    else:\n",
        "        return 'Poor (<580)'\n",
        "\n",
        "data['Credit_Rating'] = data['Score'].apply(get_rating)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "rating_order = ['Poor (<580)', 'Fair (580-669)', 'Good (670-739)',\n",
        "               'Very Good (740-799)', 'Excellent (800+)']\n",
        "sns.violinplot(x='Credit_Rating', y='Score', data=data,\n",
        "              order=rating_order, palette='RdYlGn')\n",
        "plt.title('Credit Score Distribution by Rating Category')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "rating_dist = data['Credit_Rating'].value_counts(normalize=True).loc[rating_order]\n",
        "ax = rating_dist.plot(kind='bar', color=sns.color_palette('RdYlGn', len(rating_order)))\n",
        "plt.title('Credit Rating Distribution')\n",
        "plt.xlabel('Credit Rating')\n",
        "plt.ylabel('Percentage')\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()*100:.1f}%',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10),\n",
        "                textcoords='offset points')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "corr = data[numeric_features + ['Score']].corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "           center=0, vmin=-1, vmax=1)\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "pd.Series(model_performance).sort_values().plot(kind='barh', color='skyblue')\n",
        "plt.title('Model Performance Comparison (AUC)')\n",
        "plt.xlabel('AUC Score')\n",
        "plt.xlim(0.5, 1.0)\n",
        "for i, v in enumerate(pd.Series(model_performance).sort_values()):\n",
        "    plt.text(v + 0.01, i, f\"{v:.3f}\", color='black', va='center')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "data.to_csv('enhanced_scored_customers.csv', index=False)\n",
        "\n",
        "print(\"Enhanced credit scoring pipeline completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdejR6iu0J8B",
        "outputId": "c99c9552-a028-4c2e-95e4-0ed550b438ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d4027b0da307>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_theme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"husl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MQk8VCpz6np"
      },
      "outputs": [],
      "source": []
    }
  ]
}